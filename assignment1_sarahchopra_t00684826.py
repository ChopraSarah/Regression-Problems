# -*- coding: utf-8 -*-
"""Assignment1_SarahChopra_T00684826.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KM-nGhwBm1TBdUkZ7RIpri1_aRmcxVhX

## **Assigment 1: Theoretical Machine Learning**
### Sarah Chopra
### T00684826

## **The Regression Problem = Life Expectancy ~ Schooling + GDP**

### We are trying to establish a relationship between Life Expectancy and Schooling + GDP. We have assumed that better Schooling (more number of hours contributed to schooling) and good GDP for a country might not be a good predictor for Life Expectancy. Our Null Hypothesis dictate that GDP and schooling have no relationship with Life expectancy.

### **Null Hypothesis $H_{o}$ : $β_{1}$ = 0, $β_{2}$ = 0**
### $β_{1}$ = 0 Represents Slope of Schooling
### $β_{2}$ = 0 Represents Slope of GDP

### Our alternate hypothesis dictaate that their is a relationship between Life Expectancy and Schooling + GDP
### **Alternate Hypothesis $H_{a}$ : Atleast one $H_{i}$ <> 0**

### Input = Schooling, GDP
### Output = Life Expectancy
### Life Expectancy is the average time a person is expected to live. Schooling might be a good predictor because more number of school hours could mean a more sensible, educated way of living and might contribute to a better value for life expectancy.

### At the same time, GDP tells about how good a country is doing in terms of economy. Countries with higher value of GDP may have higher standard of living, have better access to healthcare. Therefore, high GDP may indicate a better living style and better value of life expectancy.
"""

pip install pandasql

import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import seaborn as sn
from pandasql import sqldf
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn import preprocessing
import statistics
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import LeaveOneOut
from google.colab import files
import io

file = files.upload()

"""# **Data Quality:**


1.   **The column names poses an issue as they have white spaces. Example: 'Life expectancy ' (An extra space post expectancy). This is taken care of in the program.**


```
     '''Data Quality: Index(['Country', 'Year', 'Status', 'Life expectancy ', 'Adult Mortality',
       'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B',
       'Measles ', ' BMI ', 'under-five deaths ', 'Polio', 'Total expenditure',
       'Diphtheria ', ' HIV/AIDS', 'GDP', 'Population',
       ' thinness  1-19 years', ' thinness 5-9 years',
       'Income composition of resources', 'Schooling'],
      dtype='object')'''
```


**Code takes care of this:**

```
data_raw = data_raw.rename(columns=lambda x: x.strip())
```



2.   **The trailing white spaces post removing, doesnot take care of spaces between words in the name of column. Example: 'Life expectancy' (Space between Life expectancy). This is an SQL violation, code tkae care of this:**


**Code takes care of this: (Repalces ' '  with '_' an underscore.**


```
data_raw.columns = data_raw.columns.str.replace(' ', '_')```

**Visualisation:**

> **Fig 1: Life Expectancy ~ Schooling**


> **Fig 2: Life Expectancy ~ GDP**
"""

data_raw = pd.read_csv(io.StringIO(file['Life Expectancy Data.csv'].decode('latin-1')))
data_raw = data_raw.rename(columns=lambda x: x.strip()) # Correct the column names to remove whitespaces.
data_raw.columns = data_raw.columns.str.replace(' ', '_') #Remove empty space between different words in column names eg: "Life expectancy" => "Life_expectancy" for SQL compatibility
plt.figure(figsize=(30,30))

figure, axis = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))

axis[0].scatter(data_raw["Schooling"], data_raw["Life_expectancy"],c ='teal',s=30)
axis[1].scatter(data_raw["GDP"], data_raw["Life_expectancy"],c ='darkslategrey',s=30)
axis[0].set_xlabel("Schooling",fontsize=15)
axis[0].set_ylabel("Life Expectancy",fontsize=15)
axis[1].set_xlabel("GDP",fontsize=15)
axis[1].set_ylabel("Life Expectancy",fontsize=15)

"""**Visualisation:**
> **Fig 3: Correlation Between all the columns**

**Correlation between columns was used to determine the best columns.**
"""

plt.figure(figsize=(50,50))
correlationBetweenColumns = (data_raw.corr()) #Understanding Data in terms of how correlated each column is to each other.
print(correlationBetweenColumns)
sn.heatmap(correlationBetweenColumns, annot=True)  #Please maximize the size for image to view the values.

"""As it can be seen from data:



> 1) There are NaN values in all the columns. **Solution: Replacing NaN with mean of that column (grouping by country)**

> 2) There are lot of countries for which the schooling values are zero. **Solution: 1) Removed all values where schooling is zero. 2) Scaled schooling column around mean.**


> 3) There are lot of countries for which GDP values are zero.**Soultion: 1) Removed all values where GDP is 0. 2) Scaled GDP column around mean.**

In order to fix these issues following strep were taken:





"""

filteredColumns = data_raw.filter(items=['Life_expectancy', 'percentage expenditure','GDP'])

GDPMeanCountryWise = data_raw[["GDP","Country"]].groupby(["Country"]).mean().reset_index().fillna(0) # Fill 0 for countries which have NaN values for all the years in GDP COLUMN
SchoolingMeanCountryWise = data_raw[["Schooling","Country"]].groupby(['Country']).mean().reset_index().fillna(0) # Fill 0 for countries which have NaN values for all the years in GDP COLUMN
LifeExpectancyMeanCountryWise = data_raw[["Life_expectancy","Country"]].groupby(['Country']).mean().reset_index().fillna(0) # Fill 0 for countries which have NaN values for all the years in GDP COLUMN
pd.set_option('display.max_columns', None)

##The below code is responsible for replacing Nan's with mean of that column for a particular country. #NaN for a year for a particular country is set to 0.

pysqldf = lambda q: sqldf(q, globals())
cond_join= '''
    select
        data_raw.year,
        data_raw.Country,
        COALESCE(data_raw.GDP, GDPMeanCountryWise.GDP) as GDP,
        COALESCE(data_raw.Life_expectancy, LifeExpectancyMeanCountryWise.Life_expectancy) as Life_expectancy,
        COALESCE(data_raw.Schooling, SchoolingMeanCountryWise.Schooling) as Schooling
    from data_raw
    join GDPMeanCountryWise
      on
        data_raw.Country = GDPMeanCountryWise.Country
    join SchoolingMeanCountryWise
      on
        data_raw.Country = SchoolingMeanCountryWise.Country
    join LifeExpectancyMeanCountryWise
      on
        data_raw.Country = LifeExpectancyMeanCountryWise.Country
'''

regressionColumns = pysqldf(cond_join)
regressionColumns = (regressionColumns[regressionColumns["Schooling"] != 0])
regressionColumns = (regressionColumns[regressionColumns["GDP"] != 0])
y = regressionColumns['Life_expectancy'].values
x_vars = regressionColumns[["Schooling","GDP"]].values
x_vars = preprocessing.scale(x_vars,with_mean=True)
X1_train, X1_test, y_train, y_test = train_test_split(x_vars,y, test_size=0.2)
X1_train = X1_train.reshape(-1,2)
X1_test = X1_test.reshape(-1,2)
y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

"""**It can be seen from plots that GDP values revolve around 0 a lot. Therefore, calculate mean ,median ,mode for GDP to understand the data better. It can be see Mean = -0.067, Mode = -0.50554, Median = -0.4083. Hence, data is clustered around 0 and these metrices prove it.text**"""

#Plot after removing zeroes, NaN's, and post standardizing.
figure, axis = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))
axis[0].scatter(X1_test[:,0], y_test,c ='teal',s=30)
axis[1].scatter(X1_test[:,1], y_test,c ='darkslategrey',s=30)
#print(statistics.mean(X1_test[:,1])) #To understand why the majority of data points are clustered near 0. Mean of GDP = =-0.067
#print(statistics.mode(X1_test[:,1])) #-0.50554
#print(statistics.median(X1_test[:,1])) # -0.4083


axis[0].set_xlabel("Schooling",fontsize=15)
axis[0].set_ylabel("Life Expectancy",fontsize=15)
axis[1].set_xlabel("GDP",fontsize=15)
axis[1].set_ylabel("Life Expectancy",fontsize=15)

plt.show()

"""The plot of Life Expectancy Vs Schooling shows that for a positive value for life expectancy, we have corresponding schooling values 0. This does not mean that those countries have no schooling, but, it means that we couldnt collect data for schooling for that country. Hence, it would be better to ignore value 0  of schooling and procced with a positive values."""

from pandas.core.algorithms import mode
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn import svm
from mpl_toolkits.mplot3d import Axes3D

regressionTask = LinearRegression()

regressionTask.fit(X1_train, y_train)
r2_score = regressionTask.score(X1_test,y_test)

print("Im im the R^2", r2_score)
#0.33831349674958877 ---Old Model before removing zeroes.
#0.5054348342665609 ---New Model after removing zeroes. in Schooling
#0.6121338265004121 --After standrdaizing both
y_pred = regressionTask.predict(X1_test)
print("MSE for the model: " , mean_squared_error(y_test,y_pred))
print("RMSE for the model: " , np.sqrt(mean_squared_error(y_test,y_pred)))

"""***Algorithm Performance:***

Using R^2 as metric to check how good the algorithm is, below are details with different combinations:


> Model 1)  R^2, When zeroes from schooling and GDP were not removed.


> **0.33831349674958877**

> Model 2)  R^2, When zeroes from schooling and GDP were removed.

> **0.5054348342665609**

> Model 3)  R^2, Afte Standardizing the columns


> **0.6121338265004121**

> Using MSE as the metrics for model performance.

> **34.997944826548924**


"""

loo = LeaveOneOut()
loo.get_n_splits(X1_test)
i,j = 9,2
#plot_me = [[0 for x in range(j)] for y in range(i)]

plot_me = np.zeros(shape=(8,2))
l = 0
for i in {5,10,15,20,100,200,350,390}:
  crossvalidation = KFold(n_splits=i, random_state=None, shuffle=False)
  scores = cross_val_score(regressionTask, X1_test, y_test, scoring="neg_mean_squared_error", cv=crossvalidation,
  n_jobs=1)
#print(scores)
  print("Folds: " + str(len(scores)) + ", MSE: " + str(np.mean(np.abs(scores))) + ", STD: " + str(np.std(scores)))
  plot_me[l][0] = str(np.mean(np.abs(scores)))
  plot_me[l][1] = i
  l=l+1
  print(l)

plt.plot(plot_me[:,1],plot_me[:,0],'ro')
plt.xlabel("Distinct  values for k")
plt.ylabel("Average MSE for k")

plt.show()

"""**For Validation, using Leave on out. It can be seen, that as number folds increase MSE increases. **

# ***Methods used to increase performance:***

### 1.   Repalced NaN's with average of that column per country.
### 2.   Removed zeroes (GDP 0, doesnt make sense)
### 3.   Used Leave one out and can see that MSE with these features can be brought down to around 3%.
### 4.   Standardized the data to make schooling and GDP unitless, as they can now be compared.
### 5.  Iteratively checked  k ~ Average MSE.
"""